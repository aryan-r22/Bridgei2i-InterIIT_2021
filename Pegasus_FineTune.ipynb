{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pegasus_FineTune.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"rXvpaBziW_ev","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616297285128,"user_tz":-330,"elapsed":18692,"user":{"displayName":"Aryan Rastogi","photoUrl":"https://lh5.googleusercontent.com/-nVSEfsguxb0/AAAAAAAAAAI/AAAAAAAABFY/VIqKcFI98mQ/s64/photo.jpg","userId":"09704969149968468747"}},"outputId":"85a92c57-70af-4a43-fd44-47b46de58bed"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CaVFYjiEZbyJ","colab":{"base_uri":"https://localhost:8080/","height":606},"executionInfo":{"status":"ok","timestamp":1616297450420,"user_tz":-330,"elapsed":149338,"user":{"displayName":"Aryan Rastogi","photoUrl":"https://lh5.googleusercontent.com/-nVSEfsguxb0/AAAAAAAAAAI/AAAAAAAABFY/VIqKcFI98mQ/s64/photo.jpg","userId":"09704969149968468747"}},"outputId":"d45f6335-1e3f-4cfa-fddc-d79e9adb00b2"},"source":["!pip install transformers==3.5.0\n","!pip install torch==1.4.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers==3.5.0 in /usr/local/lib/python3.7/dist-packages (3.5.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (0.0.43)\n","Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (0.9.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (20.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (3.0.12)\n","Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (0.1.91)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (4.41.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (3.12.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.0) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.0) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.0) (7.1.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.5.0) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.0) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.0) (1.24.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers==3.5.0) (54.1.2)\n","Collecting torch==1.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/3b/fa92ece1e58a6a48ec598bab327f39d69808133e5b2fb33002ca754e381e/torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4MB)\n","\u001b[K     |████████████████████████████████| 753.4MB 17kB/s \n","\u001b[31mERROR: torchvision 0.9.0+cu101 has requirement torch==1.8.0, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: torchtext 0.9.0 has requirement torch==1.8.0, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n","\u001b[?25hInstalling collected packages: torch\n","  Found existing installation: torch 1.8.0+cu101\n","    Uninstalling torch-1.8.0+cu101:\n","      Successfully uninstalled torch-1.8.0+cu101\n","Successfully installed torch-1.4.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"b0L_OJov3LN6"},"source":["import os\n","import time\n","import datetime\n","import torch\n","from torch.utils.data import Dataset\n","\n","\n","\n","import argparse\n","import logging\n","import os\n","import random\n","\n","import numpy as np\n","\n","import torch\n","from torch.utils.data import DataLoader\n","\n","from transformers import (\n","    AdamW,\n","    AutoConfig,\n","    AutoModelWithLMHead,\n","    AutoTokenizer,\n","    get_linear_schedule_with_warmup,\n",")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0XZYLqCEZWPz"},"source":["from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n","import torch\n","\n","model_name = 'google/pegasus-cnn_dailymail'\n","torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","tokenizer = PegasusTokenizer.from_pretrained(model_name)\n","model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EPjChL4et4Q0"},"source":["torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1FC3UMiBuXBS"},"source":["from transformers.modeling_bart import shift_tokens_right"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w3bQ0BUKuW8a"},"source":["def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-0G-w0KO08Gy"},"source":["import tensorflow as tf\n","train_dataset_fp = tf.keras.utils.get_file('/content/gdrive/MyDrive/H2_B2I_14/Preprocessed_Dataset_Summarization.csv','Preprocessed_Dataset_Summarization.csv')\n","batch_size = 2\n","\n","train_dataset = tf.data.experimental.make_csv_dataset(\n","    train_dataset_fp,\n","    batch_size,\n","    select_columns = ['text','ctext'],\n","    num_epochs=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DHZSH8Qn7mPL"},"source":["for i,batch in enumerate(train_dataset):\n","  break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hoT9OXyh4f3Z"},"source":["def label_smoothed_nll_loss(lprobs, target, epsilon, ignore_index=-100):\n","    \"\"\"From fairseq\"\"\"\n","    if target.dim() == lprobs.dim() - 1:\n","        target = target.unsqueeze(-1)\n","    nll_loss = -lprobs.gather(dim=-1, index=target)\n","    smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n","    if ignore_index is not None:\n","        pad_mask = target.eq(ignore_index)\n","        nll_loss.masked_fill_(pad_mask, 0.0)\n","        smooth_loss.masked_fill_(pad_mask, 0.0)\n","    else:\n","        nll_loss = nll_loss.squeeze(-1)\n","        smooth_loss = smooth_loss.squeeze(-1)\n","\n","    nll_loss = nll_loss.sum()  # mean()? Scared to break other math.\n","    smooth_loss = smooth_loss.sum()\n","    eps_i = epsilon / lprobs.size(-1)\n","    loss = (1.0 - epsilon) * nll_loss + eps_i * smooth_loss\n","    return loss, nll_loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Ja_tewL59oH"},"source":["num_train_epochs = 5\n","#model.resize_token_embeddings(len(tokenizer))\n","weight_decay =0.0\n","learning_rate = 1e-4 \n","adam_epsilon = 1e-8\n","warmup_steps = 0\n","t_total = (607276 // 2 ) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VRzl54I-5isw"},"source":["no_decay = [\"bias\", \"LayerNorm.weight\"]\n","optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": weight_decay,\n","        },\n","        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n","    ]\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n","scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n","    )\n","if(os.path.exists('/content/gdrive/MyDrive/H2_B2I_14/Pegasus')):\n","  optimizer.load_state_dict(torch.load(os.path.join('/content/gdrive/MyDrive/H2_B2I_14/Pegasus', 'optimizer.pt')))\n","  scheduler.load_state_dict(torch.load(os.path.join('/content/gdrive/MyDrive/H2_B2I_14/Pegasus', 'scheduler.pt')))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D6_vnDas5yiP"},"source":["output_dir='/content/gdrive/MyDrive/H2_B2I_14/Pegasus'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eKLJmHXQ0-qs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616303675429,"user_tz":-330,"elapsed":5856419,"user":{"displayName":"Aryan Rastogi","photoUrl":"https://lh5.googleusercontent.com/-nVSEfsguxb0/AAAAAAAAAAI/AAAAAAAABFY/VIqKcFI98mQ/s64/photo.jpg","userId":"09704969149968468747"}},"outputId":"7a6e197a-a349-4589-ec9f-08ac978c737a"},"source":["pad_token_id = tokenizer.pad_token_id\n","epochs = 5\n","for epoc in range(epochs):\n","  t0 = time.time()\n","  print(\"\")\n","  print('======== Epoch {} ========'.format(epoc+1))\n","  model.train()\n","  total_train_loss = 0\n","  for i,batch in enumerate(train_dataset):\n","    title = []\n","    body = []\n","    for item in batch['text'].numpy():\n","      title.append(item.decode('utf-8'))\n","    for item in batch['ctext'].numpy():\n","      body.append(item.decode('utf-8')) \n","\n","    batch_tokens = tokenizer.prepare_seq2seq_batch(body,title,max_length=320,max_target_length=60,truncation=True,padding='max_length').to(torch_device)\n","    decoder_input_ids = shift_tokens_right(batch_tokens['labels'], pad_token_id)\n","    outputs = model(batch_tokens['input_ids'], attention_mask=batch_tokens['attention_mask'], decoder_input_ids=decoder_input_ids, use_cache=False)\n","    lm_logits = outputs[0]\n","    lprobs = torch.nn.functional.log_softmax(lm_logits, dim=-1)\n","    loss, nll_loss = label_smoothed_nll_loss(\n","                lprobs, batch_tokens['labels'],0.1, ignore_index=pad_token_id\n","            )\n","    total_train_loss += loss.item()\n","  \n","    optimizer.zero_grad()\n","    loss.backward()\n","    #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","    optimizer.step()\n","    scheduler.step()\n","    if i%500 == 0:\n","      print(\"Batch :\" + str(i)+ \"\\tTraining Loss(Un-normalized_Additive): \" +str(total_train_loss))\n","    if (i+1) % 10000 == 0:\n","        model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","        model_to_save.save_pretrained(output_dir)\n","        tokenizer.save_pretrained(output_dir)\n","        torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n","        torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt')) \n","  training_time = format_time(time.time() - t0)\n","  avg_train_loss = total_train_loss / 1050994\n","  print(\"Training loss for epoch \"+str(epoc+1)+\": \"+str(avg_train_loss))\n","  print(\"time : {}\".format(training_time))\n","  \n","  model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","  model_to_save.save_pretrained(output_dir)\n","  tokenizer.save_pretrained(output_dir)\n","  torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n","  torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 ========\n","batch :0Training Loss: 110.03550720214844\n","batch :500Training Loss: 58093.88905334473\n","batch :1000Training Loss: 116078.9450263977\n","batch :1500Training Loss: 174389.08918762207\n","traing loss epoch1:0.19262331801715735\n","time : 0:17:07\n","\n","======== Epoch 2 ========\n","batch :0Training Loss: 109.35237884521484\n","batch :500Training Loss: 57003.09384918213\n","batch :1000Training Loss: 115199.2033996582\n","batch :1500Training Loss: 171935.46877288818\n","traing loss epoch2:0.1892860587697253\n","time : 0:17:14\n","\n","======== Epoch 3 ========\n","batch :0Training Loss: 128.6055908203125\n","batch :500Training Loss: 56279.194396972656\n","batch :1000Training Loss: 111771.41904449463\n","batch :1500Training Loss: 168799.98818588257\n","traing loss epoch3:0.18576116651559138\n","time : 0:17:10\n","\n","======== Epoch 4 ========\n","batch :0Training Loss: 107.75408935546875\n","batch :500Training Loss: 55529.86949920654\n","batch :1000Training Loss: 111503.86494445801\n","batch :1500Training Loss: 166717.5108795166\n","traing loss epoch4:0.18309492945983455\n","time : 0:17:10\n","\n","======== Epoch 5 ========\n","batch :0Training Loss: 136.38113403320312\n","batch :500Training Loss: 54559.35231399536\n","batch :1000Training Loss: 108791.52241897583\n","batch :1500Training Loss: 163883.17770767212\n","traing loss epoch5:0.18038316209534253\n","time : 0:17:12\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FFY_PdSUNbbf"},"source":[""],"execution_count":null,"outputs":[]}]}